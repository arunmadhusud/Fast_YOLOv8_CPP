{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Installation"
      ],
      "metadata": {
        "id": "i8LBZPsY0ldH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ded3rSO75tmx"
      },
      "outputs": [],
      "source": [
        "!pip install ultralytics\n",
        "!pip install onnxconverter-common"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Export"
      ],
      "metadata": {
        "id": "rPSNJoD_00mI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import cv2\n",
        "import urllib.request\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torchvision.transforms as transforms\n",
        "from ultralytics import YOLO\n",
        "import torchvision\n",
        "\n",
        "# Load the YOLO model\n",
        "model = YOLO('/content/yolov8n.pt')"
      ],
      "metadata": {
        "id": "ahnQ7egB7xil"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a YOLOv8 model\n",
        "model = YOLO(\"yolov8n.pt\")\n",
        "\n",
        "# Export the model\n",
        "model.export(format=\"onnx\", opset=17, simplify=True, dynamic=False, imgsz=640)"
      ],
      "metadata": {
        "id": "CUHUEScW6cbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !yolo export model=yolov8n.pt imgsz=640 format=openvino\n"
      ],
      "metadata": {
        "id": "eVTwgX1pbwY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocess the model before quantization"
      ],
      "metadata": {
        "id": "arUie5tfv0pF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m onnxruntime.quantization.preprocess --input yolov8n.onnx --output yolov8n_infer.onnx\n"
      ],
      "metadata": {
        "id": "BEI84fXTHDaR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Static Quantization - QOperator"
      ],
      "metadata": {
        "id": "giPt5z0vwFla"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/EliSchwartz/imagenet-sample-images.git"
      ],
      "metadata": {
        "id": "CbxL9xKLAepM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/imagenet-sample-images/.git"
      ],
      "metadata": {
        "id": "B-Hh7-6iAnw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from onnxruntime.quantization import CalibrationDataReader, quantize_static, QuantType, QuantFormat\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "class ImageCalibrationReader(CalibrationDataReader):\n",
        "    def __init__(self, image_dir, num_samples=100, input_name=\"images\"):\n",
        "        # Get all image files\n",
        "        self.image_files = [\n",
        "            os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "        ]\n",
        "        # Randomly sample images\n",
        "        if len(self.image_files) > num_samples:\n",
        "            self.image_files = random.sample(self.image_files, num_samples)\n",
        "\n",
        "        self.idx = 0\n",
        "        self.input_name = input_name\n",
        "\n",
        "    def preprocess(self, image_path):\n",
        "        # Open and resize image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = image.resize((640, 640), Image.Resampling.BILINEAR)\n",
        "\n",
        "        # Convert to numpy and normalize\n",
        "        image_np = np.array(image).astype(np.float32) / 255.0\n",
        "\n",
        "        # HWC to CHW format\n",
        "        image_np = np.transpose(image_np, (2, 0, 1))\n",
        "\n",
        "        # Add batch dimension\n",
        "        image_np = np.expand_dims(image_np, axis=0)\n",
        "        return image_np\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.idx >= len(self.image_files):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            input_data = self.preprocess(self.image_files[self.idx])\n",
        "            self.idx += 1\n",
        "            return {self.input_name: input_data}\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {self.image_files[self.idx]}: {str(e)}\")\n",
        "            self.idx += 1\n",
        "            return self.get_next()"
      ],
      "metadata": {
        "id": "sdgoRK-YA7aO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calibration_data_reader = ImageCalibrationReader(\n",
        "    image_dir=\"/content/imagenet-sample-images\",  # Directory containing your images\n",
        "    num_samples=100  # Number of images to use for calibration\n",
        ")"
      ],
      "metadata": {
        "id": "hIUl843lBM__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For more accurate results, keep the operation in the postprocessing subgraph in floating point precision, using the nodes_to_exlude parameter. You can visualize the model graph using https://netron.app for finding the names of nodes in in the postprocessing subgraph\n",
        "\n"
      ],
      "metadata": {
        "id": "O8Djr0I-vp_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Nodes to exclude\n",
        "nodes_to_exclude = [\n",
        "    '/model.22/Concat_3', '/model.22/Split', '/model.22/Sigmoid',\n",
        "    '/model.22/dfl/Reshape', '/model.22/dfl/Transpose', '/model.22/dfl/Softmax',\n",
        "    '/model.22/dfl/conv/Conv', '/model.22/dfl/Reshape_1', '/model.22/Slice_1',\n",
        "    '/model.22/Slice', '/model.22/Add_1','/model.22/Add_2',  '/model.22/Sub', '/model.22/Div_1',\n",
        "    '/model.22/Concat_4', '/model.22/Mul_2', '/model.22/Concat_5'\n",
        "]\n",
        "\n",
        "# Perform static quantization\n",
        "try:\n",
        "    quantize_static(\n",
        "        model_input='yolov8n_infer.onnx',\n",
        "        model_output=\"yolov8n_st_quant.onnx\",\n",
        "        weight_type=QuantType.QInt8,\n",
        "        activation_type=QuantType.QUInt8,\n",
        "        calibration_data_reader=calibration_data_reader,\n",
        "        quant_format=QuantFormat.QOperator,\n",
        "        nodes_to_exclude=nodes_to_exclude,\n",
        "        per_channel=False,\n",
        "        reduce_range=True,\n",
        "    )\n",
        "    print(\"Quantization completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Quantization failed: {str(e)}\")"
      ],
      "metadata": {
        "id": "B9aclYVOH9HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Static Qunatization -  QDQ"
      ],
      "metadata": {
        "id": "M5vA6xlJwNpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from onnxruntime.quantization import CalibrationDataReader, quantize_static, QuantType, QuantFormat\n",
        "from PIL import Image\n",
        "import os\n",
        "import random\n",
        "\n",
        "class ImageCalibrationReader(CalibrationDataReader):\n",
        "    def __init__(self, image_dir, num_samples=100, input_name=\"images\"):\n",
        "        # Get all image files\n",
        "        self.image_files = [\n",
        "            os.path.join(image_dir, f) for f in os.listdir(image_dir)\n",
        "            if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
        "        ]\n",
        "        # Randomly sample images\n",
        "        if len(self.image_files) > num_samples:\n",
        "            self.image_files = random.sample(self.image_files, num_samples)\n",
        "\n",
        "        self.idx = 0\n",
        "        self.input_name = input_name\n",
        "\n",
        "    def preprocess(self, image_path):\n",
        "        # Open and resize image\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "        image = image.resize((640, 640), Image.Resampling.BILINEAR)\n",
        "\n",
        "        # Convert to numpy and normalize\n",
        "        image_np = np.array(image).astype(np.float32) / 255.0\n",
        "\n",
        "        # HWC to CHW format\n",
        "        image_np = np.transpose(image_np, (2, 0, 1))\n",
        "\n",
        "        # Add batch dimension\n",
        "        image_np = np.expand_dims(image_np, axis=0)\n",
        "        return image_np\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.idx >= len(self.image_files):\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            input_data = self.preprocess(self.image_files[self.idx])\n",
        "            self.idx += 1\n",
        "            return {self.input_name: input_data}\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {self.image_files[self.idx]}: {str(e)}\")\n",
        "            self.idx += 1\n",
        "            return self.get_next()"
      ],
      "metadata": {
        "id": "zCjb_GPrtwwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "calibration_data_reader = ImageCalibrationReader(\n",
        "    image_dir=\"/content/imagenet-sample-images\",  # Directory containing your images\n",
        "    num_samples=100  # Number of images to use for calibration\n",
        ")"
      ],
      "metadata": {
        "id": "zsrIm_FYtzIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Nodes to exclude\n",
        "nodes_to_exclude = [\n",
        "    '/model.22/Concat_3', '/model.22/Split', '/model.22/Sigmoid',\n",
        "    '/model.22/dfl/Reshape', '/model.22/dfl/Transpose', '/model.22/dfl/Softmax',\n",
        "    '/model.22/dfl/conv/Conv', '/model.22/dfl/Reshape_1', '/model.22/Slice_1',\n",
        "    '/model.22/Slice', '/model.22/Add_1','/model.22/Add_2',  '/model.22/Sub', '/model.22/Div_1',\n",
        "    '/model.22/Concat_4', '/model.22/Mul_2', '/model.22/Concat_5'\n",
        "]\n",
        "\n",
        "# Perform static quantization\n",
        "try:\n",
        "    quantize_static(\n",
        "        model_input='yolov8n_infer.onnx',\n",
        "        model_output=\"yolov8n_st_quant_qdq.onnx\",\n",
        "        weight_type=QuantType.QInt8,\n",
        "        activation_type=QuantType.QUInt8,\n",
        "        calibration_data_reader=calibration_data_reader,\n",
        "        quant_format=QuantFormat.QDQ,\n",
        "        nodes_to_exclude=nodes_to_exclude,\n",
        "        per_channel=False,\n",
        "        reduce_range=True,\n",
        "    )\n",
        "    print(\"Quantization completed successfully!\")\n",
        "except Exception as e:\n",
        "    print(f\"Quantization failed: {str(e)}\")"
      ],
      "metadata": {
        "id": "LjYVfjaWtkcQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dynamic Qunatization"
      ],
      "metadata": {
        "id": "40C3bpxTv6Vv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
        "\n",
        "input_model_path = 'yolov8n.onnx'\n",
        "output_model_path = 'yolov8n_dy_quant.onnx'\n",
        "quantize_dynamic(\n",
        "    model_input=input_model_path,\n",
        "    model_output=output_model_path,\n",
        "    weight_type=QuantType.QUInt8,\n",
        "    reduce_range=True\n",
        ")"
      ],
      "metadata": {
        "id": "jtJKBCabEHKa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Float 16 conversion"
      ],
      "metadata": {
        "id": "8uZ-6fEcv_9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import onnx\n",
        "from onnxconverter_common import float16\n",
        "\n",
        "# Load the model\n",
        "model_fp32 = onnx.load(\"yolov8n.onnx\")\n",
        "\n",
        "# Convert to FP16\n",
        "model_fp16 = float16.convert_float_to_float16(model_fp32)\n",
        "\n",
        "# Save the FP16 model\n",
        "onnx.save(model_fp16, \"yolov8n_fp16.onnx\")\n",
        "\n",
        "print(\"Converted yolov8n.onnx to yolov8n_fp16.onnx\")\n"
      ],
      "metadata": {
        "id": "jCmHwQAPa2fb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import onnxruntime as ort\n",
        "# import numpy as np\n",
        "# from PIL import Image\n",
        "# import time\n",
        "\n",
        "# def preprocess_image(image_path):\n",
        "#     \"\"\"Loads and preprocesses an image for inference.\"\"\"\n",
        "#     image = Image.open(image_path).convert('RGB')\n",
        "#     image = image.resize((640, 640), Image.Resampling.BILINEAR)\n",
        "\n",
        "#     # Convert to numpy and normalize\n",
        "#     image_np = np.array(image).astype(np.float32) / 255.0\n",
        "\n",
        "#     # HWC to CHW format\n",
        "#     image_np = np.transpose(image_np, (2, 0, 1))\n",
        "\n",
        "#     # Add batch dimension\n",
        "#     return np.expand_dims(image_np, axis=0)\n",
        "\n",
        "# def run_inference(model_path, input_tensor):\n",
        "#     \"\"\"Runs inference on the given ONNX model and measures time taken.\"\"\"\n",
        "#     session = ort.InferenceSession(model_path, providers=['CPUExecutionProvider'])\n",
        "#     input_name = session.get_inputs()[0].name\n",
        "\n",
        "#     # Start time measurement\n",
        "#     start_time = time.time()\n",
        "\n",
        "#     # Run inference\n",
        "#     outputs = session.run(None, {input_name: input_tensor})\n",
        "\n",
        "#     # End time measurement\n",
        "#     end_time = time.time()\n",
        "\n",
        "#     # Calculate time taken\n",
        "#     time_taken = end_time - start_time\n",
        "#     return outputs, time_taken\n",
        "\n",
        "# # Load test image\n",
        "# image_path = \"/content/imagenet-sample-images/n01440764_tench.JPEG\"  # Replace with your test image\n",
        "# input_tensor = preprocess_image(image_path)\n",
        "\n",
        "# # Run inference on original model\n",
        "# orig_outputs, orig_time = run_inference(\"yolov8n.onnx\", input_tensor)\n",
        "\n",
        "# # Run inference on quantized model\n",
        "# quant_outputs, quant_time = run_inference(\"static_quantized.onnx\", input_tensor)\n",
        "\n",
        "# # Print output and timing results\n",
        "# print(\"Original Model Output:\", orig_outputs[0].flatten()[:10])  # Print first 10 values\n",
        "# print(\"Quantized Model Output:\", quant_outputs[0].flatten()[:10])  # Print first 10 values\n",
        "\n",
        "# # Compute mean absolute difference\n",
        "# difference = np.abs(orig_outputs[0] - quant_outputs[0])\n",
        "# print(f\"Mean Absolute Difference: {np.mean(difference)}\")\n",
        "\n",
        "# # Print time taken for each model\n",
        "# print(f\"Time taken for original model: {orig_time:.4f} seconds\")\n",
        "# print(f\"Time taken for quantized model: {quant_time:.4f} seconds\")\n"
      ],
      "metadata": {
        "id": "2M7M2q9Fb5SW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}